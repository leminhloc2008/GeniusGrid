import cv2
import numpy as np
from deepface import DeepFace
import random
import time
import tensorflow as tf
import threading
from PIL import Image, ImageDraw, ImageFont
from gtts import gTTS
import pygame
import tempfile
import os
import queue

# GPU Configuration
print("S·ªë GPU kh·∫£ d·ª•ng: ", len(tf.config.experimental.list_physical_devices('GPU')))
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "GPU v·∫≠t l√Ω,", len(logical_gpus), "GPU logic")
    except RuntimeError as e:
        print(e)
else:
    print("Kh√¥ng t√¨m th·∫•y GPU. ƒêang ch·∫°y tr√™n CPU.")

# List of emotions and corresponding emojis
emotions = {
    "t·ª©c gi·∫≠n": "üò†", "gh√™ t·ªüm": "ü§¢", "s·ª£ h√£i": "üò®",
    "h·∫°nh ph√∫c": "üòä", "bu·ªìn b√£": "üò¢", "ng·∫°c nhi√™n": "üò≤", "b√¨nh th∆∞·ªùng": "üòê"
}

# Success time for each emotion (in seconds)
emotion_success_times = {
    "t·ª©c gi·∫≠n": 2.0, "gh√™ t·ªüm": 1.5, "s·ª£ h√£i": 1.5,
    "h·∫°nh ph√∫c": 2.0, "bu·ªìn b√£": 2.0, "ng·∫°c nhi√™n": 1.5, "b√¨nh th∆∞·ªùng": 2.5
}

# Detailed guides for each emotion in Vietnamese
emotion_guides = {
    "t·ª©c gi·∫≠n": [
        "Nh√≠u m√†y m·∫°nh, k√©o l√¥ng m√†y l·∫°i g·∫ßn nhau",
        "M√≠m m√¥i ho·∫∑c ƒë∆∞a h√†m d∆∞·ªõi ra",
        "M·ªü to m·∫Øt, nh√¨n th·∫≥ng"
    ],
    "gh√™ t·ªüm": [
        "NhƒÉn m≈©i, k√©o m√¥i tr√™n l√™n",
        "Nheo m·∫Øt m·ªôt ch√∫t",
        "Nghi√™ng ƒë·∫ßu ra sau m·ªôt ch√∫t"
    ],
    "s·ª£ h√£i": [
        "M·ªü to m·∫Øt, k√©o l√¥ng m√†y l√™n",
        "H√© mi·ªáng m·ªôt ch√∫t",
        "Ng·∫£ ng∆∞·ªùi ra sau n·∫øu c√≥ th·ªÉ"
    ],
    "h·∫°nh ph√∫c": [
        "C∆∞·ªùi r·ªông, ƒë·ªÉ l·ªô rƒÉng",
        "Nh∆∞·ªõn m√†y l√™n m·ªôt ch√∫t",
        "ƒê·ªÉ m·∫Øt s√°ng l√™n, c√≥ n·∫øp nhƒÉn ·ªü ƒëu√¥i m·∫Øt"
    ],
    "bu·ªìn b√£": [
        "K√©o kh√≥e mi·ªáng xu·ªëng",
        "Nh√≠u m√†y, t·∫°o n·∫øp nhƒÉn gi·ªØa hai l√¥ng m√†y",
        "H·∫° √°nh m·∫Øt xu·ªëng, c√≥ th·ªÉ nh√¨n xu·ªëng d∆∞·ªõi"
    ],
    "ng·∫°c nhi√™n": [
        "M·ªü to m·∫Øt, n√¢ng l√¥ng m√†y cao",
        "H√© mi·ªáng, h·∫° h√†m d∆∞·ªõi xu·ªëng m·ªôt ch√∫t",
        "T·∫°o n·∫øp nhƒÉn tr√™n tr√°n"
    ],
    "b√¨nh th∆∞·ªùng": [
        "Th·∫£ l·ªèng c√°c c∆° m·∫∑t",
        "Gi·ªØ mi·ªáng kh√©p, nh∆∞ng kh√¥ng ch·∫∑t",
        "Nh√¨n th·∫≥ng v·ªõi √°nh m·∫Øt b√¨nh tƒ©nh"
    ]
}

# Colors for different types of text
COLORS = {
    'target': (255, 105, 180),  # H·ªìng ƒë·∫≠m
    'success': (0, 255, 0),  # Xanh l√°
    'error': (0, 0, 255),  # ƒê·ªè
    'info': (255, 165, 0)  # Cam
}


class AudioManager:
    def __init__(self):
        pygame.mixer.init()
        self.speech_queue = queue.Queue()
        self.current_speech = None
        self.interrupt_flag = threading.Event()
        self.speech_thread = threading.Thread(target=self._process_speech_queue, daemon=True)
        self.speech_thread.start()

    def speak(self, text, interrupt=False):
        if interrupt:
            self.interrupt_flag.set()
            with self.speech_queue.mutex:
                self.speech_queue.queue.clear()
        self.speech_queue.put(text)

    def _process_speech_queue(self):
        while True:
            text = self.speech_queue.get()
            if text is None:
                break
            self._play_speech(text)

    def _play_speech(self, text):
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as fp:
                tts = gTTS(text=text, lang='vi')
                tts.save(fp.name)
                self.current_speech = fp.name

            pygame.mixer.music.load(self.current_speech)
            pygame.mixer.music.play()

            while pygame.mixer.music.get_busy():
                if self.interrupt_flag.is_set():
                    pygame.mixer.music.stop()
                    self.interrupt_flag.clear()
                    break
                pygame.time.Clock().tick(10)

            os.unlink(self.current_speech)
            self.current_speech = None
        except Exception as e:
            print(f"L·ªói khi ph√°t √¢m thanh: {e}")

    def stop(self):
        self.speech_queue.put(None)
        self.speech_thread.join()
        pygame.mixer.quit()


# Kh·ªüi t·∫°o AudioManager
audio_manager = AudioManager()


def speak(text, interrupt=False):
    audio_manager.speak(text, interrupt)


# Load a font that supports Vietnamese
font = ImageFont.truetype("arial.ttf", 16)


def cv2_im_to_pil(cv2_im):
    return Image.fromarray(cv2.cvtColor(cv2_im, cv2.COLOR_BGR2RGB))


def pil_to_cv2_im(pil_im):
    return cv2.cvtColor(np.array(pil_im), cv2.COLOR_RGB2BGR)


def draw_text_with_pil(cv2_im, text, position, font, text_color=(255, 255, 255)):
    pil_im = cv2_im_to_pil(cv2_im)
    draw = ImageDraw.Draw(pil_im)
    draw.text(position, text, font=font, fill=text_color)
    return pil_to_cv2_im(pil_im)


def display_emotion_and_feedback(frame, target_emotion, feedback, detected_emotion=None):
    """Hi·ªÉn th·ªã c·∫£m x√∫c m·ª•c ti√™u v√† ph·∫£n h·ªìi tr√™n khung h√¨nh"""
    height, width, _ = frame.shape

    text = f"H√£y th·ªÉ hi·ªán c·∫£m x√∫c: {emotions.get(target_emotion, '?')} ({target_emotion})"
    frame = draw_text_with_pil(frame, text, (10, 15), font, COLORS['target'])

    y_offset = 45
    color = COLORS['success'] if "Tuy·ªát v·ªùi" in feedback else COLORS['error']
    for line in feedback.split('\n'):
        frame = draw_text_with_pil(frame, line, (10, y_offset), font, color)
        y_offset += 22

    if detected_emotion:
        text = f"C·∫£m x√∫c ph√°t hi·ªán: {emotions.get(detected_emotion, '?')} ({detected_emotion})"
        frame = draw_text_with_pil(frame, text, (10, height - 30), font, COLORS['info'])

    return frame


def analyze_emotion(frame):
    """Ph√¢n t√≠ch c·∫£m x√∫c s·ª≠ d·ª•ng DeepFace"""
    try:
        with tf.device('/GPU:0'):
            result = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)
        detected_emotion = result[0]['dominant_emotion']
        # Map English emotion to Vietnamese
        emotion_map = {
            "angry": "t·ª©c gi·∫≠n", "disgust": "gh√™ t·ªüm", "fear": "s·ª£ h√£i",
            "happy": "h·∫°nh ph√∫c", "sad": "bu·ªìn b√£", "surprise": "ng·∫°c nhi√™n",
            "neutral": "b√¨nh th∆∞·ªùng"
        }
        return emotion_map.get(detected_emotion, "unknown")
    except Exception as e:
        print(f"L·ªói khi ph√¢n t√≠ch c·∫£m x√∫c: {e}")
        return "unknown"


def give_feedback(expected_emotion, detected_emotion):
    """ƒê∆∞a ra ph·∫£n h·ªìi d·ª±a tr√™n c·∫£m x√∫c ph√°t hi·ªán ƒë∆∞·ª£c"""
    if expected_emotion == detected_emotion:
        feedback = f"Tuy·ªát v·ªùi! B·∫°n ƒë√£ th·ªÉ hi·ªán ch√≠nh x√°c c·∫£m x√∫c {expected_emotion}."
    elif detected_emotion == "unknown":
        feedback = "Kh√¥ng th·ªÉ ph√°t hi·ªán c·∫£m x√∫c. H√£y th·ª≠ l·∫°i v√† ƒë·∫£m b·∫£o khu√¥n m·∫∑t c·ªßa b·∫°n ƒë∆∞·ª£c nh√¨n th·∫•y r√µ r√†ng."
    else:
        feedback = f"B·∫°n ƒëang th·ªÉ hi·ªán {detected_emotion}, kh√¥ng ph·∫£i {expected_emotion}.\n"
        feedback += "H√£y th·ª≠ l·∫°i v·ªõi nh·ªØng g·ª£i √Ω sau:\n"
        for guide in emotion_guides.get(expected_emotion, []):
            feedback += f"- {guide}\n"

    speak(feedback)
    return feedback


def congratulate(emotion, duration):
    """ƒê∆∞a ra l·ªùi ch√∫c m·ª´ng chi ti·∫øt h∆°n"""
    congratulation = f"Xu·∫•t s·∫Øc! B·∫°n ƒë√£ gi·ªØ ƒë∆∞·ª£c bi·ªÉu c·∫£m {emotion} trong {duration:.1f} gi√¢y. "
    congratulation += "Kh·∫£ nƒÉng ki·ªÉm so√°t khu√¥n m·∫∑t c·ªßa b·∫°n th·∫≠t ·∫•n t∆∞·ª£ng. "
    congratulation += "H√£y ti·∫øp t·ª•c ph√°t huy v√† th·ª≠ th√°ch b·∫£n th√¢n v·ªõi c·∫£m x√∫c ti·∫øp theo!"

    print(congratulation)
    speak(congratulation, interrupt=True)

    congrats_image = np.zeros((200, 400, 3), dtype=np.uint8)
    congrats_image = draw_text_with_pil(congrats_image, "Ch√∫c m·ª´ng!", (30, 30), font, (0, 255, 0))
    congrats_image = draw_text_with_pil(congrats_image, f"B·∫°n ƒë√£ th·ªÉ hi·ªán {emotion}", (30, 70), font, (255, 255, 255))
    congrats_image = draw_text_with_pil(congrats_image, f"trong {duration:.1f} gi√¢y!", (30, 110), font, (255, 255, 255))
    congrats_image = draw_text_with_pil(congrats_image, "L√†m t·ªët l·∫Øm!", (30, 150), font, (0, 255, 255))

    cv2.imshow("Ch√∫c m·ª´ng!", congrats_image)
    cv2.waitKey(5000)  # Hi·ªÉn th·ªã trong 5 gi√¢y
    cv2.destroyWindow("Ch√∫c m·ª´ng!")


def emotion_analysis_thread(frame_queue, result_queue):
    """Lu·ªìng ph√¢n t√≠ch c·∫£m x√∫c"""
    while True:
        frame = frame_queue.get()
        if frame is None:
            break
        emotion = analyze_emotion(frame)
        result_queue.put(emotion)


def main():
    print("Ch√†o m·ª´ng ƒë·∫øn v·ªõi Tr√≤ ch∆°i Nh·∫≠n di·ªán C·∫£m x√∫c!")
    speak("Ch√†o m·ª´ng ƒë·∫øn v·ªõi Tr√≤ ch∆°i Nh·∫≠n di·ªán C·∫£m x√∫c!")
    print("Nh·∫•n 'q' ƒë·ªÉ tho√°t tr√≤ ch∆°i b·∫•t c·ª© l√∫c n√†o.")
    speak("Nh·∫•n q ƒë·ªÉ tho√°t tr√≤ ch∆°i b·∫•t c·ª© l√∫c n√†o.")

    cv2.namedWindow('Tr√≤ ch∆°i Nh·∫≠n di·ªán C·∫£m x√∫c', cv2.WINDOW_NORMAL)
    cv2.resizeWindow('Tr√≤ ch∆°i Nh·∫≠n di·ªán C·∫£m x√∫c', 800, 600)

    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    frame_queue = queue.Queue(maxsize=1)
    result_queue = queue.Queue()

    analysis_thread = threading.Thread(target=emotion_analysis_thread, args=(frame_queue, result_queue))
    analysis_thread.start()

    target_emotion = random.choice(list(emotions.keys()))
    speak(f"H√£y th·ªÉ hi·ªán c·∫£m x√∫c {target_emotion}")

    success_time = None
    last_frame_time = time.time()
    detected_emotion = None
    feedback = "H√£y b·∫Øt ƒë·∫ßu th·ªÉ hi·ªán c·∫£m x√∫c!"

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        current_time = time.time()
        fps = 1 / (current_time - last_frame_time)
        last_frame_time = current_time

        # C·∫≠p nh·∫≠t frame cho ph√¢n t√≠ch c·∫£m x√∫c
        if frame_queue.empty():
            frame_queue.put(frame)

        # Ki·ªÉm tra k·∫øt qu·∫£ ph√¢n t√≠ch c·∫£m x√∫c
        if not result_queue.empty():
            detected_emotion = result_queue.get()

            if target_emotion == detected_emotion:
                if success_time is None:
                    success_time = current_time
                elif current_time - success_time >= emotion_success_times[target_emotion]:
                    duration = current_time - success_time
                    congratulate(target_emotion, duration)
                    target_emotion = random.choice(list(emotions.keys()))
                    speak(f"B√¢y gi·ªù, h√£y th·ª≠ th·ªÉ hi·ªán {target_emotion}")
                    success_time = None
                    feedback = "H√£y b·∫Øt ƒë·∫ßu th·ªÉ hi·ªán c·∫£m x√∫c m·ªõi!"
            else:
                success_time = None
                feedback = give_feedback(target_emotion, detected_emotion)

        # Hi·ªÉn th·ªã th√¥ng tin tr√™n frame
        frame = display_emotion_and_feedback(frame, target_emotion, feedback, detected_emotion)
        cv2.putText(frame, f"FPS: {fps:.2f}", (10, frame.shape[0] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

        cv2.imshow('Tr√≤ ch∆°i Nh·∫≠n di·ªán C·∫£m x√∫c', frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # D·ªçn d·∫πp
    frame_queue.put(None)
    analysis_thread.join()
    cap.release()
    cv2.destroyAllWindows()
    print("C·∫£m ∆°n b·∫°n ƒë√£ ch∆°i!")
    speak("C·∫£m ∆°n b·∫°n ƒë√£ ch∆°i!")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"ƒê√£ x·∫£y ra l·ªói: {e}")
    finally:
        audio_manager.stop()
        pygame.quit()